<html>
    <head>
        <title>
            Audio &amp; Video
        </title>
    </head>
    <body>
        <h1>Video</h1>
        <h2>Texture mapped triangles: Rasterizer for a memory with a databus 4 times as wide as a px and a technology node which allow a hardware multiplier on die.</h2>
        <h3>Alignment and fast page mode</h3>
        <p>Triangle rasterization works best line by line. A framebuffer most of the time is organized line by line for easy readout to the screen.
            All external memory has byte granularity for write enable. So lets assume that we have at least byte sized pixel ( otherwise we would need a read, modify, write cycle for unaligned boundaries ).
            So we write the px to an internal register. When it is full or we reached the end of the line, we write it out to the frame buffer.
            Fast page mode sounds like some artifical constraint, but it is (was real) and gives 2 times speed boost.
            For it we need to enlarge our register to hole multiple words. I assume that our shader is slow or needs the memory. A simple fill does not need this extra step.
        </p>
        <p>
            Now for each pixel we load a texel. To give equal performance to a rotated texture it should be possible to align the pixel in a square. And I assume mipmapping or at least a matching texture resolution for the scene.
            Thus consecutive px-reads will probably read from the same word. Thus we keep this word in the source register. In the corners we will probably read the same word even in the next scanline.
            If a triangle is skewed, we render each line into the skew direction. For low amount of skew, we render zig zag and can reuse the source register at each U-turn.
        </p>
        <p> For longer lines the source register needs to become larger. Going with the spirit of the Jaguar, we don't want to lose performance in the worst case: Zoom out without mimaps .. probably anisotrophic.
            There the cache does not work. Thus we don't read ahead. The cache simply stores address (20 bit)+dirty flag + value (64 bit). Each new read address is put on the bus .. a fan out to all 16 entry slots.
            Each compares the address to each own and -- if it machtes -- answers on the data bus. There is a pointer for new entries. It just rans round robin for each write into cache ( each miss ).
        </p>
        <p>
            Now on shared memory ( Jaguar, Atari ST, N64, C64, or multiple shaders ) with the complicated bus arbitration and also due to our beloved fast page mode, it would be better if the texture reads happen in a burst.
            Technology in 1993 could have used an external SRAM chip to copy the texture to and then a z-buffer. But we now assume that we try to render out of a large texture atlas residing in DRAM.
            So I would store the address in the cache, but don't read, but mark as dirty. Hits are also marked as protected. The write pointer jumps over protected cells and waits before a dirty cache cell and issues a burst read instead.
            The rasterizer now performs a second pass over the pixels since the last read-burst and now replace the placeholders ( pointer into cache cell and designation of the px withing) with the real value.
        </p>
        <p>
            Now with wide triangles we may lose our cache entries some time after the U-turn.
            We could split large triangles in 3 dimensions, but espcially architecture has large flat surfaces and there is a cheaper way for splitting.
            We take the overlap of two lines and trim to alignment borders.
            If there is still overlap we can render the top 4 px and then the lower 4 px.
            For this the inner loop needs to know how to navigate into another scanline, but with linear interpolation for both end points, this is not complex.
            We still end up with a lots of gradients, but keep in mind that we already have a 16 entry cache and most processors have 16 registers.
            So we need gradients for R,G,B ( Gouraud) one texture ( U,V ) and just to be sure another texture (S,V). (B)lend ? So 8 gradients for horizontal interpolation alone.
            Now with vertical -- on both sides for compatibility with Doom ) we end up with 24 gradients. Thus our system is still blanced, technically.
            Now the page size on the framebuffer is so that one has to tweak the adressing ( on most configurations)
            to keep vertically neighboring pixels in the same page. But it has be done usually and is no big deal.
            We can expand this pattern up to 8 lines.
        </p>
        <p>
            For our vertical interpolation we need to store the coordinates of the last line .. so like all those 8 values.
            Now we are in the outer loop and timing is not critical, but storage need chip area.
            Let's say that we store 8 points.
        </p>
        <h3>Division and multiplication</h3>
        <p> For small triangles which are not more wide than 16px or more heigh, the best bang for the buck is to linearly interpolate between the vertices like it was parallel projection (affine transformation).
            The triangle can even be larger if it is pointing towards the camera</p>
            <p>
                All others apply the perspective correction. The correct math ends up -- after a long journey -- in a shape, where
                we have to interpolate once, and then divide all our coordinates by Z.
                So being too precise has never helped in computer graphics and we already use up a lot of memory and we can afford to
                store W=1/Z with many fractional bits but still a small rounding error. We make up for this with extre precise vertex shaders.
                Now at each px we need to multiply our coordinates with this.
                So division costs 16 cycles max ( 32 bit int ), or you apply Newton Raphsody on a guess and only need 2 multiplications.
                The guess is either the last evaluation ( use some heuristics for this choice),
                or we scan the bits, convert to float, lookup the inverse for the top bits of the mantissa ( all 8 bit ), like PSX did.
                Fortunately we need the inverse anyway without varying nominator. 
            </p>
            <p>
                As already motivated for the small triangles, we don't need nor want to do this for every px.
                For the span mode mentioned above, we partition the line into 16px chunks. The remainder is placed on the far end where more correction is needed.
                We could even say that we often don't have all gradients or only a single texture and basically 8px should be enough for 8 MULs in the background.                
            </p>
            <p>
                When we draw two lines at once, bilinear interpolation 4x2 px is possible. For two textures and colored light and blended mipmap we
                may profit from 4x4 px . Notice however that the edges are still drawn as spans .. on both sides with a probability of 3/4.
                And since we allow 8px spans and think that spans below 4px are a waste, 4x4 blocks are surly sandwiched between spans ( extreme Skew ? ).
            </p>
            <p> The perspective correction again needs 24 register plus 3 for Z (W) . The interpolation before the correction works just like the original interpolation. Get the vectors of the edges, invert matrix, and use fraction to translate to the center of the pixels.
                Notice how we don't need to repeat the last step after correction because we never leave the centers of the px again.
                It would be possible to interpolate along the edge. I mean I want to avoid extreme values for the correction because they pull in errors. Perspective correction on a point outside of the triangle is not allowed for me.
                It should be possible to do this corrction right on the edge. That could happen with pixels anyway. Maybe include some saturation logic.
                Thus it is possible to use affine texture mapping for the corners ( middle corner make sense when we copy the long edge by multiples of 16px towards it until it cuts out a small triange.).
                Triangles can be used to fill the area on the sides of the 4x4 pattern. Since I probably don't have that much area and 4x4 speeds up the frame as a whole anyway .. I guess,
                the shorter code wins. Triangulation is quite ugly ( breaks symmetry ) and the perspective correction does not happen on a px I actually use, but on the edge next to it.
                So in the end Ids Quake span interpolation wins. Descent also used it. Only on systems with nerfed division, one would try alternatives, though .. 
            </p>
            <p>
                Now I think I like perspective correction on the edge. Linear interpolation on the line to a px for example of a 4x4 block needs a division, but it still only needs one division ( precision may need on Newton Raphsody),
                to interpolate from a pixel of the 4x4 towards the edge.
                Not that this does not work in the corners. 
                It does work on the middle corner though. And top an bottom also when you slice vertically. The sub triangles have one edge on the original triangle, but one of the co-ordinates has no fraction.
                Thus the original triangle is not coverd in a sub-pixel sense. The rasterizer first looks for the coverd pixels and then splits.
                In one direction the first triange will have an edge with a length shorter than pixel width just to reach the edge. Ah, not really.
                For a more vertical slope, we should start with horizontal splits. Then we stack the vertical splitting triangles on top. Due the slope we don't throw so much of the edge of the first triangle away.
                Triangles can have T-joints on the pixels because there are not problems with rounding. So there is one pixel which touches an outside pixel on its corner. From this pixel two triangles, vertically and horizontally, originate.
            </p>
            <p>
                ..I want to allow for cont-z rendering like in Doom. Maybe the framebuffer should be able to be configured as 2x2 ps squares. This also helps with small triangles
                like we would use on main characters and bosses.
                And then like on the Jag triangles can be rendered horizontally, vertically, and diagonal. For 6DoF we check the normal of the plane and choose the closest of the 8 directions on screen.
                This makes this compatible with Doom and further justyfies long linear spans of 16 px. It needs a lazy output cache ( target register ) though.
                Still this cache is easy because we are sure that all output px are close to each other, not matter the zoom.
                We do not really need this because we are already perspective correct and also the multiple line mode collides with the coverage buffer in Doom and the raycaster in Wolf3d -- but hey, I love const-z.
                I has some synergy with zig zag triangulation.
            </p>
            <p>
                Const-z needs strips of finite width to look clean. Two affine triangles combined have this zizag problem which I feel is not optimal.
                To keep it linear, I propose two skewed rectangles. One is a alined to the left edge and the other to the right.
                So these extend over the opposite edges and should have zero influence there.
                So we pull edge distances from Bresenham and mix the UV coordinates using these weights? Mul is fast. It does not need much precision here, because the parallelogram should mostly agree.
                We need to scale the edge equation to reach zero at that upper, opposite vertex (once pre strip). The side edges will always be much smaller and thus the pure affine triangles corners also.
                Ah, to avoid normalization I need one blend direction: Const-z it is.
            </p>
            <p>
                So the big question is: Is there a way to blend over the affine texture mapping,
                which is just so great for small triangles, to a large triangle with a tile filled interior?
                Span based rendering is the contender.
                Here you turn on perspective correction on the edges like in Doom. This mode really profits from 8 direction const-z.
                You need two division for the correction on the extreme pixel ( usually inside the edge) and one division for the interpolation.
                When then span gets longer, you insert 8px subspans. Here interpolation only needs a shift and thus this only adds one division.
                So here is a logical path.
                Only thing that hurts are the short spans near the top and bottom vertex ( corners ).
                Thus sub span rendering will grow into the triangle from the middle vertex.
                It will start with a split where we calculate the perspective correction on the edge oposing the vertex.
                Then this splits into two edge cacluations to give scanlines to the sub span renderer.
                One of the splits stays a bit to utilize the middle vertex a little more.
                As above the subspans are 8px, but the remainer is something between 6px and 14px. So it is not such a slow down.
                With slanted edges it could be possible to have 4x4 tiles internally, and then reduce to 4x1 and 8x1 tiles close to the edge.
                Then add some more leeway for the variable spans: 5..16px.
                Do some benchmarks to simplify the code.
            </p>
            <p>
                Now how do affine triangles on the side look compared to this?
                We have const z and can split the quads on the sides with the diagonal with less z variance and thus have an excuse for the split.
                Triangles only offer a speed up in conjunction with interior tiles. Thus all fully 4x4 covered squares become tiles.
                We already use triangles for top and bottom corner.
                Depending on the const-z direcation and the slope of and edges we may have to employ differnt splits on those edges.
                Thus it makes sense to just fill the 4x4 tiles without any regard to const z.
                Then on each edge the split direction ( one of the 8 or more like 4) and the triangluation split is chosen.
                In the corner we set the next edge onto the old triangluation because T joints on pixels are allowed.
                The original vertex attracts triangluatio in a fan like manner.
                Some heuristics is need to determine the reach.
            </p>
            <p>
                I feel like I could make the triangle split the winner with 8x8 tiles and (1,2) direction vectors for the splits.
                And the make a heuristic to have wider borders where the triangles make sense, like even have zig-zag (lowest lowest z variance, second lowest z variance) triangles in the corners above the tile core.
                Per triangle there is one division ( the determinant ) (we have some cheap multiplications due to the 16 directions), but for a zig or zag we need to calculate the intersection .. again, one division.
                So two division per triangle is not worse than the remainder span. Though, ah we need perspective correction at the cutting point.
            </p>
            <p>
                The main reason for 8x8 blocks is cache memory. I can easily throw transistors at the algorithmic side, but communication transisor needs tend to grow with a square law.
                So small triangles and slithers are rendered line by line and a small cache suffices. Slithers are slow on any system. So lets concentrate on triangles which should be easy and have a good fill rate.
                For a 8x8 block I calculate the deltas and choose a MIP level (mandatory) where I jump max to the next phrase (64 bit Jaguar DRAM word width).
                Each time a new word is needed I add its address to a buffer with capacity 64. The pixel points to this.
                The jump to the next line can also change the word address. Zigag meander if you don't want to remember the start. 
                The sign can change from line to line in bilinear interpolation. In N64 quad split into triangles interpolation both tris can have a different sign. I don't see why I would do the triangles individually. Does not seem to make a difference.
                This way we can trace the previous line and check if any phrase address is already in the buffer.
                This only works if we load a dense line, like slightly bigger than a hairline, more like the rays in ray-casting of Wolfenstein3d.
                Then I could also just back project the square onto the texture.
                Notice how worst case we load 64 phrases = 256 bytes which is still smaller than 4kB of cache in the N64. Maybe 4x4 is the sweet spot?
                Now I have got all phrases, but in a linearized storage. So each pixel needs to find its phrase. I think delta walking works here. I have a list of line starts.
                Interesting how backproject can create a slither in UV coordinates. Still thanks to our delta limit, there cannot be more than 8 lines.
                Backprojection is kinda expensive. Sort vertices by U. Ah, maybe here actually just running over 8x8 phrases and checking 4 inequalities is faster.
                Then again thanks to the dense lines I can finde dupes by 8(4) comparisons per pixel and only need to check 6 address bits. So 48(24) xor bits and I get the pointers into the cache in one step.
                I feel like I can weaken the constraint and allow anisotrophic scaling .. Ah there is no proof for a dense line. Just when I space along the line &lt; px aka half a phrase in 16 bit mode,
                then I need to space &gt; sqrt(2) along the other direction. Seems like I need to maintain a dense line separately from the sample positions, just as if I would cache for whole scanlines.
                Or I need to check back two previous lines or just all? So the constraints don't help much?
                Relax constraints a bit and have 4+4=8 bit word addresses. 4x4 px in framebuffer give 16 * 8 bit data to compare. I don't see a way to utilize the triangular shape.
                So it is basically 15*8 bit compares, which can run in sync with address generation. The fractional address is regenerated or uhm, for nearest pixel and minimu 16 colors these are only 4 bits.
            </p><p>
                Triangles up to 16 pixels are drawn using the same hardware. Most triangles to fill up the margin around the 4x4 blocky part fit into this.
                With slopes near the axes, the triangles need to also be split into blocks along with the bulk area.
            </p><p>
                Can I reuse values from the previous block in a brute force fashion? New address must take precedence. Basically, I can only use the used capacity of the buffer and only if the move wasn't too large.
                Otherwise I would need to copy values and also not reach all of them. Also think about the increase in cases for the automated tests.
                If I draw block on block the last values should match the first in the next block. So I need an offset into the cache with wrap around. I think this is okay if I have two bits per entry:
                value is valid, address is in queue. And values from the last block still need to be copied. This is a least-recently-used cache implementation where the 64bit value is almost as cheap to copy as the address.
                Also the addresses are only valid in a window. Now how do I know which addresses run out of the window? Do I rather have full 32bit (24bit) addresses? Or 16 bit.
                I think that on 2 MB RAM hardware we can insist on a max texture size of 512kB. 
                This is interesting when zooming into the high detail mip map level. Full meander?
            </p><p>
                    With affine triangle halves I know if x or y direction on screen is shorter in UV coordinates. So I could go along the short axis and be sure that the long axis only needs to check the last line.
                    This affects the memory buffer if I do it full polygon, but can be hidden if done inside blocks.
                    8x8 blocks and each triangle starts at one of the corners. 4x4 block has a diagonal which certainly should be communicated from the first half to the second.
                    So here is asymmetry. Then I can also start one triangle at the wide side including the diagonal and the other on the corner. 8bits to check if diagonal value is valid.                            
            </p><p>
                Bilinear interpolation is cheap because I just resuse the already cached pixels.
                I can see how N64 rather used explicit TextureMemmory. If I allow 16 color format with palette, 4k is plenty. Demos show how the cartridge is the limit.
                Or just go along the line and check address changes and cache the output as the Jaguar can do almost well. 
            </p><p>
                It now seems clar to me that 8bi hardware can only draw affine sprites. Already the C64 liked to zoom into sprites. So we cache the source.
                Thanks to affinity, the lines always point in the same direction and we set up the buffer accordingly. Lines hava a slope %lt; 45° and are 2words high to encompas the actual scanline.
            </p>
        <h2>Video</h2>
        <p> 320x240px.
            DAC: RGB 444 ( period correct for CGA sort of )
            8x8 tiles with 1 or 2 colors.
            Smooth scroll 0..7px ( push; delay buffer ). Rough scroll ( pull; memory start address, pitch, wrap bit depth;).
            Rough is bufferd: Atomic write for all by setting smooth 0XXX0YYY reg.
            4 layers aka z-values.
            1 byte/px (1 byte RLE for transparency / Color Keying) sprite overlay with these
            color formats: RGBI 2222 + 3 cycles of 223
            Also possible: combine with background to generate RGB 333.
            16 sprites per scanline. Unlimited height and width due to realtime data loading into 9bit/pixel FIFO
        </p>
        <p>
            Integer zoom is used to create large bosses within the memory bandwidth limit. Fractional zoom using Bresenham is
            used for Hang Out billboards. Also the street is created using zoomed horizontal spans. With the camera fixed on the track, texture is mostly aligned to the scanlines, but does not need to be rigidly.
            The X-positions of texture V ordinate crossings can be calculated on the CPU and with all sprites zoom-able, like 4 crossings are realistic. So the street need not be pinned,
            but true 3d calculation can be used.  
        </p>
        <p>
            In this kind of game all sprites overlap on a scanline. 16 sprites suffices already for hangout, but not out run.
            Anyway, the simple byte buffer format allows the CPU to software render the far away spot into a frame buffer. Scenery in these old games clung to the street.
        </p>
        <p>
            
        Euclidean algorithm with rounding to nearest should result in the delta pattern
        which is most prevalent. So with each iteration we at least halve the pixels where we need to think.
        Long spans feel like different sprites. So small rotations could be better be done in the CPU.
        On the other hand Bresenham is also used for edges, so these may be calculated in the GPU ( maybe in horizontal retrace? 
        </p>
        <h3> Display BSP aka the Beam Tree </h3>
        <p>
            Raytracing and portal rendering feel like dead ends. Polygon rendering into a z-buffer always works. We still can use a tree to allow us to scale.
            So we have scene graph for the scene. This allows us to have (large) vehicles, which can move and rotate. A BSP and spheres can rotate and translate
            and thus are ideal geometry four bounding shapes. Bounding shapes of scene graph ( scene tree ) children overlap. For the spheres there exists a polygonal time algorithm for a tight fit.
            A BSP is an optimization problem which needs itertion and heuristics and logs.
            The root of the BSP feels a bit strange, so let us consider the case that we already cut out a convex shape (not necessaryly simplex??.
            Now we look at the bounding box like a module in programming: With imports and exports.
            So we export a 2d BSP for the projection through the volume with all the polygons which belong to this scene graph.
            All volumes behind ( non-overlapping ) are occluded by this. I guess that this is not a portal because other occluders are also considered.
            Occluders win, (inofficial) portals lose.
            As we move up in the scene graph ( in order to render front to back ), we merge the BSP trees from the children.
            I see no other watertight way for a polygon based scene ... espcially not the spheres, which would be come disks on a spherical "fisheye lense" projection.
            With hirachical-z one would have to decide on a stop. Those jaggies just feel so ugly when all you need to do instead
            are two Muls and one compare on a CPU with decent amount of bits.
            To dive into a convex shape we take the BSP light pattern on the parent shape and apply all sibling occluders which are not strictly behind.
            This visiblity pattern is so to say the parameter for the method invocation into the child.
        </p>
        <p>
            BSP merge takes the large node and applies the cut to the destination (we basically build a fresh tree).
            We still mark the origin of each cut. If we insert a convex region and it needs to be cut by a foreing cut,
            we post pone this cut until a child* also wants to cut us. If the other tree reaches its leafs, and those have different opacy,
            we may need to catch up. Due to sortingf by area, not much harm has been done, yet.
            And now we can discard our occluded details. We only cut nodes and visible leafs.
        </p>
        <p>
            Now, is there some clean-up possible? Proof? Unit test. I think I designed above splitter to avoid unecessary cuts.
            How many adopting parents can a region have?
            It feels weird to merge more than 2 BSP at once, but for early occlusion this may help.
            Maybe keep the occluded area in an indexed aggregate and use transparent area for sort?
            Looking at the 3d case: can we cache combinations of merged trees ( on the up path anyway),
            but also with overlap with foreign scene graphs. So calc most intense overlap and cache?
            We calc shadow on the way up.
        </p>
        <h2>Rounding errors and nearest pixel fall back</h2>
        <p>If I aim for the Jag with 16bit factors or even for peace of mind even on 32 bit GBA,
            the algorithm should not crash on rounding errors.
            For frame to frame coherence and to avoid z-fighting,
            I already thought up that a 3d polyhedron exports its 2d shiloutte ( watertight 2d BSP) into a range of directions.
            So any tests on inequality of some multiply-accumulation needs to be done for extreme values.
            So for the real render on screen, px wiggle is important on 16 bit systems.
            I could either declare the BSP as truth and avoid any gaps due to wiggle ( leafs in BSP are always filled).
            Or I could keep in pixel rasterizatio as fallback for high detail nodes which get too small on screen.
            With leafs around 8px, it is best to just render them into the z-buffer. I could even use local ( to scene graph ) coordinates.
            With smaller leafs, ray casting might be interesting. The nice thing about these is the floating point rounding local to
            scene graph and BSP node, and there is not wiggle of vertices. Rather the rays get somewhat irregularyl.
            We don't mind because affine texture mapping and Gouraud are already an approximation.
            Wiggle on screen can be small with local screen coordinates before multiplication ( add is 32 bit anyway ).
            8x8 px blocks can be affine: So you have the perspective bounding polyhedron projected on screen.
            3d coordinates are tri-linearly interpolated. Feels like a gimmick, in this case. I don't see how this prevents leaks.            
        </p>
        <p> With homogenous coordinates, how difficult is it to calculate ranges?
            Wiggle on screen ( which we minimize ), gives a range to each vertex position. The camera position has an error.
            So for translation, these add up. With rotation ( in a scene graph ), each inner product consists of many products.
            Each product needs to combine 2 * 2 borders and get min-max. Then min of the sum is sum of min ( wax likewise ).
            Intersection tests add the cross product. Maybe, we get titghter limits and less branching going with the determinant. 
        </p>
        <p>
            When I used the z-buffer rasterizer for an area, I may want to collect the number of occluded pixels for my aggregation.
            Maybe, all pixels are indeed covered? I guess a loop with a hanging branch can do this for me at 10 cycles per pixel.
            Not great, so only for small details.
        </p>
        <h2>Rounding is necessary for BSP</h2>
        <p>So I start with the 2d case. To order intersections, we often need the sign of the area of a triangle which is created
        by intersection of three half spaces each defined by two points. Thus for the intersection we solve a linear system of equations.
        In the 2d case we get one multiplication in the denominator.
        The denominator is an inner product with one flipped vector. So for two cuts on the same edge, we sure can factor out the inner product.
        We don't calculate the area, we only want to sort the cuts along some meaningful ordinate ( round rotate to cube face).
        Now with the determinant it feels like we multiply all denominators together before the add?
        So we only have 2 muls and need to calculate the sign of the denominator?
        The Jag can give me the sign of a 16bit*16bit multiplication. So we need 8 bit numbers: Local grid in a scene graph.
        This BSP breaks down for smooth vehicle movement.
        Or we calculate ranges anyway and round after every multiplication. Maybe even float stuff.
        Multiplication is nice for ranges because we only need the product of all min(abs()) and all max(abs()).
        For less bits (6502, or 2x2 digits on JRIS) one can distribute factors out.
        The camera vector enters into the calculation preceding the 2d case.
        It ends up as a polynom. Here just assume worst case and independent orders.
        (Area is nasty and only for heuristics: To calculate the determinant (area, cross * inner) of the triangle, I need a common denominator and thus 2 more multiplications.)
        </p>
        <p>
          Now in 3d cuts of planes which result in edges mean two equations with 3 unknown. Then Gauss Jordan
          to elliminate one of the ordinates per equation. Make those dependent on the common ordinate?
          We then want to order cuts along this edge. So we have to inverter a 3x3 matrix and thus have a determinant in the denominator with two multiplications.
          Though we can factor out an inner product if we order along on edge because two planes stay the same.
        </p>
        <p>
            With the bottom-up approach inside a node we have a camera vector. Level of Detail, persistence, and occlusion is the motivation for the beam tree, so of course,
            there is a field of view and the camere vector has a length. So there is really no special vector inside a node.
            Already with low precision and a scope view ( not in MVP ), default transformation should work. We use the direction only for occlusion. For rendering, do gaps exist?
            Linear interpolation bends our straigh lines and may lead to gaps in the BSP tree. So we apply error margins here. Or BSP rules. Only vertices are projected. A vertex is always assgined to a single node.
            Overlap may be a problem here.
        </p>
        <p>
            I want rounding be solved for persistence, and to avoid artifacts on retro systems.
            I guess that nobody else cares. So I step in.
            Since we use perspective projection, there is no real 3d geometric interpretation of the ranges.
            When we reach the machine precision, stuff cannot be decided.
            So I guess I fall back to a polygon mesh with z-buffer in screen space?
            Or I try a different cut which is not on the tipping point?
            It should no happen often, but does lead to complexity explosion in my BPS tree
            because merge stops. Or I need variable precision? I want statistics!
            Efficient BSP tree operation is the corner stone of the beam tree.
            Then again, how pathologic needs a case to be that multiple BSP calculation steps fail?
            Moire or lines of trees ?
            When a merge aborts, basically we just don't get infor about watertight occlusion
            because the object is so ill defined and full of holes like swiss cheese.
            It would be interesting to test. Spikes in 16 bit precision?
            Just harden to code. No decision possible? Don't calc the overlap and fall back to z-buffer.
        </p>
        <h3>draft</h3>
        <p>
            So it really comes down to the question: How efficient is this merge?
            It will mix some levels in the tree.
            If I cut right through all the polygons in the other tree, the number of cuts grows linear with (sqrt with number of polygons).
            We do cut, but only consider regions of similar size. So "divorced" parents are created.            
        </p>
        <p>
            How to speed up? One idea which fits my liking of high fps and static scenes (levle in a sim)
            is to persist data across frames. For every calculation I calculate an error margin for the camera position.
            I only dive into trees which exceed the margin.
            But I this frame to frame coherence has to take a back seat in graphics for games.
            Is there any way I can avoid the cuts, but still get a watertight result?
            Can I allow for overlap like I do with scene graph -> 3d BSP? Something like max 8 cells overlap at one point?
            So we have the top level cuts of both trees and then mark the children in the other trees that they overlap with two others.
            So when we enter a 2d convex region, we check all overlapping 2d occluders.
            It can happen that we need to dive into the occluders to match the area.
            The real goal is of course to combine occluders, like some part of the tree after the split
            are full of occluders,
            or are full of holes,
            or full of internal polygons.
        </p>
        <p>
            So how do we keep our overlap limit? We merge not two BSP, but already all their overlapping ancestors.
            So basically we always merge 2*8=16 BSP into 8 BSP? Maybe I should reduce that overlap number to 4 or add another limit.
            Anyway, how do choose who to combine? Similar size. No combinatoric explosion due to cuts? Similar ancestors?
            Number of cuts need to be the metric. If the data shrinks on merge we keep it. Otherwise discard. Persist statistics above frames.            
        </p>
        <p>
            We sort cuts by area. So the largest area gets it cut served into the output.
            So only large area cuts cut large area cuts.
            We need to get rid of those cuts before we move so details.
            This works because we have only two states per area ( transparant vs occluded).
            So while we move down a tree we could either wander orthogonal to the large scale cut and not be botherd anymore.
            Or we reached two leafs on both sides of the cut in the source which have the same state.
            It can even be nodes with the same color on the edge and not child edges cutting (how costly to detect?). 
            There can be quite large leafs because we want to create occluders.
            That is we design the level not as an astorioid mine field or wood in winter,
            but with compact rocks, walls, trees full of leafs.
        </p>
        <p>
            Now again, we move up in our 3d tree. How do we detect good occluders? When we generally use a class of overlapping BSP
        </p>
        <p>
            <a href="https://www.bluesnews.com/abrash/">Abrash</a>
            Sprites conjure the idea that a position is more important than the color. On CRTs sprites can be calculated at quite a high horizontal resolution, but vertical movement
            jumps per scan line. With vintage portable screens with low resolution even horizontal movement wants antialiasing. The only way to achieve this that I can live with is,
            is a BSP tree within each scanline. But this has quite high overdraw cost, so naturally I'd go all 2d display BSP, or at least a reliable front to back rendering.
        </p>
        <p>
            With reliable front to back, I mean that in case of doubt ( no 3d BSP and after backface culling ), triangles which overlap can cheaply be cut at a z-value and also have some synergy with
            perspective correct texture mapping. Final calculation of values from edge positions is quite expensive, but only needs to be done with zero overdraw.
            Also texels are already dissolved, no shader need to be loaded and the workload is steady. Hardware support comes in form of low precision mul and div combined with cheap branching in hyperthreading CPU core. 
        </p>
        <h2>Audio</h2>
        floating point matrix rotation for sine tones. The floating point is implemented
        by only acting every 1,2,4,8 cycle.
        Matrix multiplication is even done less times.
        Overtones drown in noise of 1% noise of PWM + edge shifter (diagonal)
)
    </body>
</html>